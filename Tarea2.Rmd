---
output:
  html_document: default
  pdf_document: default
  word_document: default
---
# Tarea 2
####### Introducion a las Redes Neuronales Artificiales CO-6612
####### Alimi Garmendia 14-10392
## Ejercicio 1
Es sabido que el preceptron solo puede resolver sistemas que sean linealmente separables asi que debemos demostrar que el sistema no es linealmente separable.
Por reduccion a lo absurdo, de ser linealmente separable el algoritmo del perceptron debe converger para alguna constante de aprendizaje positiva.


Comenzamos definiendo nuestros patrones y salidas esperadas asi como funciones auxiliares para el calculo.
El bias, se incluye en el vector de pesos sinapticos.
```{r}
X <- matrix(c(2,6,1,1,3,1,3,9,1),nrow=3,ncol=3)

Y <- matrix(c(0,1,1),nrow=3,ncol=1)


dot <- function(a,b){
  return(a%*%b)
}

umbral <- function(a){
  if (a > 0){
    return(1)
  }
  return(0)
}

show <- function(x,result,real){
  im <- matrix(x, ncol = 28, nrow = 28)
  image(im, col=gray((255:0)/255),xaxt='n',yaxt='n',cex=0.5,mar=c(0,4,4,2))
  title(main = c('predicted: ',result), sub = real)

}

```
Configuramos el perceptron 

```{r}

perceptron <- function(x,y,eta,iter){
  w <- rep(0, 3) #incializamos el vector de pesos en 0
  errors <- rep(0,iter) # Definimos un vector de errores por entrada
  for (j in 1:iter) {
    for (i in 1:length(y)) {
      d <- umbral(dot(X[,i],w))
      
      w <- w + eta*(y[i]-d)*X[,i]  #Actualizamos el vector de pesos

      if ((y[i] - d) != 0.0) { # si el valor predicho difiere del esperado aumentamos el valor del vector
        errors[j] <- errors[j] + 1
      }
    }
    
  }
  
  return(errors) #Deolvemos el numero de errores
}
```

Haciendo el calculo para eta = 1/2 obtenemos lo siguiente:


```{r echo=FALSE}
err <- perceptron(X,Y,1/2,50)


plot(1:50, err, type="l", lwd=2, col="red", xlab="numero de epoca", ylab="errores")


```

Como podemos ver, no hallamos un vector de pesos sinapticos que pueda realizar la asignacion deseada, el algoritmo no converge por lo tanto los datos no son linealmente separables.

Visualmente podemos confirmarlo al ver que no podemos separar por categorias los puntos usando una linea recta.

```{r echo=FALSE}
x <- c(2,1,3)
y <- c(6,3,9)
res <- c(0,1,1)
data <- data.frame(x,y,res)
library(ggplot2)
ggplot(data, aes(x,y, color=as.factor(res)))+geom_point()
```

## Ejercicio 2
```{r}

perceptron <- function(train,eta,numneur,iter){
  x <- train[,2:785] 
  #Inicializamos la matriz de pesos
  w <- rnorm(dim(x)[2]*numneur,0,0.25) #Inicializamos la matriz de pesos sinapticos con las dimensiones deseadas
  w <- matrix(w,ncol=dim(x)[2],nrow=numneur)
  for (epoch in 1:iter) {
    train <- train[sample(nrow(train)),] #Antes de cada epoca hacemos un shuffle de los datos 
    y <- train[,1]
    x <- train[,2:785]
    x <- x/255
      for (j in 1:length(y)) {
        d <- which.max(dot(w,x[j,]))
        if (d != y[j]+1){ #Si la salida calculada no es igual al valor deseado
          w[y[j]+1,] = w[y[j]+1,]+eta*x[j,] #Aumentamos la que debio ganar
          w[d,] = w[d,]-eta*x[j,] #Penalizamos la que gano
          
        }
        
      }
    
  }
  return(w)
}

```
Esta implementacion del perceptron toma el dataset train , el cual tiene en la primera columna el valor que debe tener a la salida y en las demas columnas los patrones, el eta o constante de entrenamiento, la cual debe ser mayor a cero y por ultimo el numro de epocas por las cuales se va a entrenar. La funcion retorna la matriz de pesos sinapticos w.


## Ejercicio 3

```{r echo=FALSE}
dot <- function(a,b){
  return(a%*%b)
}
umbral <- function(a){
  if (a > 0){
    return(1)
  }
  return(0)
}
# visualize the digits
show <- function(x,result,real){
  im <- matrix(x, ncol = 28, nrow = 28)
  image(im, col=gray((255:0)/255),xaxt='n',yaxt='n',cex=0.5,mar=c(0,4,4,2))
  title(main = c('predicted: ',result), sub = real)

}

perceptron <- function(train,eta,numneur,iter){
  x <- train[,2:785]
  #Inicializamos la matriz de pesos
  w <- rnorm(dim(x)[2]*numneur,0,0.07)
  w <- matrix(w,ncol=dim(x)[2],nrow=numneur)
  for (epoch in 1:iter) {
    train <- train[sample(nrow(train)),]
    y <- train[,1]
    x <- train[,2:785]
    x <- x/255
      for (j in 1:length(y)) {
        d <- which.max(dot(w,x[j,]))
        if (d != y[j]+1){
          w[y[j]+1,] = w[y[j]+1,]+eta*x[j,]
          w[d,] = w[d,]-eta*x[j,]
          
        }
        
      }
    
  }
  return(w)
}
test <- function(x,y,w){
  count <- 0
  for (i in 1:length(y)) {
    d <- which.max(dot(w,x[i,]))
    if (d-1 == (y[i])) {
      count <- count+1
    }
  }
 return(count/length(y))
  
}




train <- read.csv('mnist_train.csv')
train <- apply(train,2, as.numeric)

test_data <- read.csv('mnist_test.csv')
test_data <- apply(test_data,2, as.numeric)
y_d <- test_data[,1]


x_d <- test_data[,2:785]
x_d <- x_d/255
```

 Entrenaremos nuestro sistema por 50 epocas para eta = 0.1,0.01 y 0.001
 
```{r}
precition <- c()
elapsed <-c()
```
 
 
 
 
```{r}
 for (i in 1:3) {
  start_time <- Sys.time()
  w <-perceptron(train,1/(10**i),10,50)
  elapsed <-c(elapsed, Sys.time()- start_time)
  precition <- c(precition,test(x_d,y_d,w))
}
```
Podemos comparar la precision y el tiempo de ejecucion para los tres casos:


```{r echo=FALSE}
x <- c(0.001,0.01,0.1)
timeit <- data.frame(x,elapsed)
precitionit <- data.frame(x,precition)

ggplot(timeit,aes(x,elapsed))+
  labs(title='Representacion Tiempo vs Constante de aprendizaje',x= 'eta',y='Tiempo de ejecucion (min)')+
  geom_line(color='Darkred',size=1)+
  geom_point(color='Darkred',size=4)+
  ylim(1.5,2.5)+
  theme_minimal()

ggplot(precitionit,aes(x,precition))+
  labs(title='Representacion Precision vs Constante de aprendizaje',x= 'eta',y='Precision')+
  geom_line(color='Darkred',size=1)+
  geom_point(color='Darkred',size=4)+
  ylim(0.8,1)+
  theme_minimal()

```
 
 Podemos ver que el valor de la constate de aprendizaje no parece afectar de manera significativa el tiempo de ejecucion, sin embargo la precision parece aumentar conforme disminuye el eta.
 
Podemos representar las imagenes y verificar la prediccion del modelo:

```{r echo=FALSE}
par(mfrow=c(1,2))
for (i in sample(1:dim(x_d)[1], 2)) {
  show(x_d[i,],which.max(dot(w,x_d[i,]))-1,y_d[i])
}

```